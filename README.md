# ğŸ›¡ï¸ Guardianâ€¯AI  
### *Your Personal Multimodal Guardian â€” Powered by Google Gemmaâ€¯3n*

<p align="center">
  <img src="logo/logo1.webp" width="150" alt="Guardian AI Logo">
</p>

---

## ğŸš€ The Story Begins: *A Thought, A Promise, A Guardian*

I started with a vision: **AI that truly understandsâ€”privately and on-device**.  
In a world of cloud-first AI, **Guardianâ€¯AI flips the script**.

It listens (*audio*), sees (*images*), and converses (*text*)â€”but all **decisions, emotional cue detection, and emergency actions** are made exclusively by **Gemmaâ€¯3n**. Never outsourced. Never streamed.

While `Whisper` and `BLIP` from Hugging Face assist with **transcription** and **image captioning**,  
**Gemmaâ€¯3n** remains the ultimate arbiterâ€”**digesting all inputs and deciding**:  
> â¤ *Respond*â€ƒâ€ƒâ¤ *Nudge*â€ƒâ€ƒâ¤ *Notify Contacts*  

All under one **holistic judgment framework**.

---

## ğŸ” Why This Problem, Why Now

- ğŸ›¡ï¸ **Privacy-first imperative**:  
  *Gemmaâ€¯3n works entirely offline*. Your voice, photos, and thoughts **never leave your device**.

- ğŸ”— **Multimodal synergy**:  
  Gemmaâ€¯3n is **the only local model** to seamlessly unify *audio*, *image*, and *text*â€”offering **context-aware assistance** with zero cloud latency.

- ğŸ’¡ **Edge-ready intelligence**:  
  Built for real-world usage. This system runs comfortably on ~2â€¯GB RAM using tools like `E2B`, proving the **power of on-device AI**.

> ğŸ§  **In short**:  
> â€ƒâ€ƒ**Gemmaâ€¯3n** is the *brain*;  
> â€ƒâ€ƒ**Whisper** and **BLIP** are just its *sensory organs*.

---

## ğŸ§  Core Architecture & Decision Flow

### 1. ğŸŒ€ Preprocessing Stage
- ğŸ™ï¸ **Audio**:  
  Transcribed using `WhisperProcessor` + `WhisperForConditionalGeneration`.

- ğŸ–¼ï¸ **Image**:  
  Captioned using `BlipProcessor` + `BlipForConditionalGeneration`.

---

### 2. ğŸ§  Decision Stage
Processed inputs are unified and passed to:
```python
GuardianAI.chat()
```

* **Gemmaâ€¯3n** evaluates:

  * Emotional tone  
  * Urgency  
  * Context  

* It then returns structured JSON:

```json
{
  "Action": "nudge" | "emergency_contact",
  "Risk": "High",
  "Analysis": "Expressing self-hatred..."
}
```

---

### 3. âœ… User Empowered Control

If a critical action is recommended:

* A **confirmation prompt** is shown in the UI  
* **No action** is taken without **explicit user consent**  
* Alerts (if approved) are formatted and sent to emergency contacts

> ğŸ”’ **Gemmaâ€¯3n never delegates**.  
> `Whisper` and `BLIP` just describe;  
> **Gemmaâ€¯3n decides.**

---

## âœ¨ Hero Features

âœ… **Unified Multimodal Input**  
Text, audio, and imagesâ€”**all flow into one stream**

ğŸ§  **Gemmaâ€¯3n as the Mind**  
Every decision is **interpreted, evaluated, and acted on** by Gemmaâ€¯3n

ğŸ” **Resilient, Private, On-Device**  
No internet required. No remote trust assumptions.

ğŸ›‘ **Empowered User Confirmations**  
Critical alerts are **never auto-sent**â€”you choose.

ğŸ¨ **Stylish & Accessible UI**  
Built with Streamlit â€” featuring **dark theme**, **chat bubbles**, and **alert boxes**

---

## ğŸš§ Setup & Quick Walkthrough

> **Demo:**  
> Deployed on Streamlit using Gemmaâ€¯3n inference endpoint *(link coming soon)*.

> **Run Locally in 3 Simple Steps:**

```bash
# 1. Clone the repo and install dependencies
git clone https://github.com/your-username/guardian-ai.git
cd guardian-ai
pip install -r requirements.txt

# 2. Set the Ollama host
set OLLAMA_HOST=127.0.0.1:11502  # Windows
# or export OLLAMA_HOST=127.0.0.1:11502  # Mac/Linux

# 3. Start Ollama server
ollama serve
```

Then simply run the app via:

```bash
streamlit run app.py
```

Youâ€™re now ready to explore Guardianâ€¯AI locally âœ¨

---

## â¤ï¸ Final Thoughts

**Guardianâ€¯AI isnâ€™t just software.**  
Itâ€™s a **narrative of trust**.

You share your voice. Your visuals.  
And **Gemmaâ€¯3n listens**â€”**safely**, **privately**, and **in full control**.

> Whisper and BLIP bring the world in.  
> **Gemmaâ€¯3n decides what matters.**  
> Thatâ€™s what makes this truly your *Guardian*.
